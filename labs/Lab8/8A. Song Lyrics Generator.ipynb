{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8A. Song Lyrics Generator\n",
    "\n",
    "In this lab, you will scrape a website to get lyrics of songs by your favorite artist. Then, you will train a model called a Markov chain on these lyrics so that you can generate a song in the style of your favorite artist.\n",
    "\n",
    "# Question 1. Scraping Song Lyrics\n",
    "\n",
    "Find a web site that has lyrics for several songs by your favorite artist. Scrape the lyrics into a Python list called `lyrics`, where each element of the list represents the lyrics of one song.\n",
    "\n",
    "**Tips:**\n",
    "- Find a web page that has links to all of the songs, like [this one](http://www.azlyrics.com/n/nirvana.html). [_Note:_ It appears that `azlyrics.com` blocks web scraping, so you'll have to find a different lyrics web site.] Then, you can scrape this page, extract the hyperlinks, and issue new HTTP requests to each hyperlink to get each song. \n",
    "- Use `time.sleep()` to stagger your HTTP requests so that you do not get banned by the website for making too many requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics = []\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the lyrics to the first song.\n",
    "print(lyrics[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pickle` is a Python library that serializes Python objects to disk so that you can load them in later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(lyrics, open(\"lyrics.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2. Unigram Markov Chain Model\n",
    "\n",
    "You will build a Markov chain for the artist whose lyrics you scraped in Lab A. Your model will process the lyrics and store the word transitions for that artist. The transitions will be stored in a dict called `chain`, which maps each word to a list of \"next\" words.\n",
    "\n",
    "For example, if your song was [\"The Joker\" by the Steve Miller Band](https://www.youtube.com/watch?v=FgDU17xqNXo), `chain` might look as follows:\n",
    "\n",
    "```\n",
    "chain = {\n",
    "    \"some\": [\"people\", \"call\", \"people\"],\n",
    "    \"call\": [\"me\", \"me\", \"me\"],\n",
    "    \"the\": [\"space\", \"gangster\", \"pompitous\", ...],\n",
    "    \"me\": [\"the\", \"the\", \"Maurice\"],\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "Besides words, you should include a few additional states in your Markov chain. You should have `\"<START>\"` and `\"<END>\"` states so that we can keep track of how songs are likely to begin and end. You should also include a state called `\"<N>\"` to denote line breaks so that you can keep track of where lines begin and end. It is up to you whether you want to include normalize case and strip punctuation.\n",
    "\n",
    "So for example, for [\"The Joker\"](https://www.azlyrics.com/lyrics/stevemillerband/thejoker.html), you would add the following to your chain:\n",
    "\n",
    "```\n",
    "chain = {\n",
    "    \"<START>\": [\"Some\", ...],\n",
    "    \"Some\": [\"people\", ...],\n",
    "    \"people\": [\"call\", ...],\n",
    "    \"call\": [\"me\", ...],\n",
    "    \"me\": [\"the\", ...],\n",
    "    \"the\": [\"space\", ...],\n",
    "    \"space\": [\"cowboy,\", ...],\n",
    "    \"cowboy,\": [\"yeah\", ...],\n",
    "    \"yeah\": [\"<N>\", ...],\n",
    "    \"<N>\": [\"Some\", ..., \"Come\"],\n",
    "    ...,\n",
    "    \"Come\": [\"on\", ...],\n",
    "    \"on\": [\"baby\", ...],\n",
    "    \"baby\": [\"and\", ...],\n",
    "    \"and\": [\"I'll\", ...],\n",
    "    \"I'll\": [\"show\", ...],\n",
    "    \"show\": [\"you\", ...],\n",
    "    \"you\": [\"a\", ...],\n",
    "    \"a\": [\"good\", ...],\n",
    "    \"good\": [\"time\", ...],\n",
    "    \"time\": [\"<END>\", ...],\n",
    "}\n",
    "```\n",
    "\n",
    "Your chain will be trained on not just one song, but by all songs by your artist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_markov_chain(lyrics):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      - lyrics: a list of strings, where each string represents\n",
    "                the lyrics of one song by an artist.\n",
    "    \n",
    "    Returns:\n",
    "      A dict that maps a single word (\"unigram\") to a list of\n",
    "      words that follow that word, representing the Markov\n",
    "      chain trained on the lyrics.\n",
    "    \"\"\"\n",
    "    chain = {\"<START>\": []}\n",
    "    for lyric in lyrics:\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "        \n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pickled lyrics object that you created in Lab A.\n",
    "import pickle\n",
    "lyrics = pickle.load(open(\"lyrics.pkl\", \"rb\"))\n",
    "\n",
    "# Call the function you wrote above.\n",
    "chain = train_markov_chain(lyrics)\n",
    "\n",
    "# What words tend to start a song (i.e., what words follow the <START> tag?)\n",
    "print(chain[\"<START>\"])\n",
    "\n",
    "# What words tend to begin a line (i.e., what words follow the line break tag?)\n",
    "print(chain[\"<N>\"][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's generate new lyrics using the Markov chain you constructed above. To do this, we'll begin at the `\"<START>\"` state and randomly sample a word from the list of words that follow `\"<START>\"`. Then, at each step, we'll randomly sample the next word from the list of words that followed each current word. We will continue this process until we sample the `\"<END>\"` state. This will give us the complete lyrics of a randomly generated song!\n",
    "\n",
    "You may find the `random.choice()` function helpful for this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_new_lyrics(chain):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      - chain: a dict representing the Markov chain,\n",
    "               such as one generated by generate_new_lyrics()\n",
    "    \n",
    "    Returns:\n",
    "      A string representing the randomly generated song.\n",
    "    \"\"\"\n",
    "    \n",
    "    # a list for storing the generated words\n",
    "    words = []\n",
    "    # generate the first word\n",
    "    words.append(random.choice(chain[\"<START>\"]))\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    # join the words together into a string with line breaks\n",
    "    lyrics = \" \".join(words[:-1])\n",
    "    return \"\\n\".join(lyrics.split(\"<N>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_new_lyrics(chain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3. Bigram Markov Chain Model\n",
    "\n",
    "Now you'll build a more complex Markov chain that uses the last _two_ words (or bigram) to predict the next word. Now your dict `chain` should map a _tuple_ of words to a list of words that appear after it.\n",
    "\n",
    "As before, you should also include tags that indicate the beginning and end of a song, as well as line breaks. That is, a tuple might contain tags like `\"<START>\"`, `\"<END>\"`, and `\"<N>\"`, in addition to regular words. So for example, for [\"The Joker\"](https://www.azlyrics.com/lyrics/stevemillerband/thejoker.html), you would add the following to your chain:\n",
    "\n",
    "```\n",
    "chain = {\n",
    "    (None, \"<START>\"): [\"Some\", ...],\n",
    "    (\"<START>\", \"Some\"): [\"people\", ...],\n",
    "    (\"Some\", \"people\"): [\"call\", ...],\n",
    "    (\"people\", \"call\"): [\"me\", ...],\n",
    "    (\"call\", \"me\"): [\"the\", ...],\n",
    "    (\"me\", \"the\"): [\"space\", ...],\n",
    "    (\"the\", \"space\"): [\"cowboy,\", ...],\n",
    "    (\"space\", \"cowboy,\"): [\"yeah\", ...],\n",
    "    (\"cowboy,\", \"yeah\"): [\"<N>\", ...],\n",
    "    (\"yeah\", \"<N>\"): [\"Some\", ...],\n",
    "    (\"time\", \"<N>\"): [\"Come\"],\n",
    "    ...,\n",
    "    (\"<N>\", \"Come\"): [\"on\", ...],\n",
    "    (\"Come\", \"on\"): [\"baby\", ...],\n",
    "    (\"on\", \"baby\"): [\"and\", ...],\n",
    "    (\"baby\", \"and\"): [\"I'll\", ...],\n",
    "    (\"and\", \"I'll\"): [\"show\", ...],\n",
    "    (\"I'll\", \"show\"): [\"you\", ...],\n",
    "    (\"show\", \"you\"): [\"a\", ...],\n",
    "    (\"you\", \"a\"): [\"good\", ...],\n",
    "    (\"a\", \"good\"): [\"time\", ...],\n",
    "    (\"good\", \"time\"): [\"<END>\", ...],\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_markov_chain(lyrics):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      - lyrics: a list of strings, where each string represents\n",
    "                the lyrics of one song by an artist.\n",
    "    \n",
    "    Returns:\n",
    "      A dict that maps a tuple of 2 words (\"bigram\") to a list of\n",
    "      words that follow that bigram, representing the Markov\n",
    "      chain trained on the lyrics.\n",
    "    \"\"\"\n",
    "    chain = {(None, \"<START>\"): []}\n",
    "    for lyric in lyrics:\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pickled lyrics object that you created in Lab A.\n",
    "import pickle\n",
    "lyrics = pickle.load(open(\"lyrics.pkl\", \"rb\"))\n",
    "\n",
    "# Call the function you wrote above.\n",
    "chain = train_markov_chain(lyrics)\n",
    "\n",
    "# What words tend to start a song (i.e., what words follow the <START> tag?)\n",
    "print(chain[(None, \"<START>\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's generate new lyrics using the Markov chain you constructed above. To do this, we'll begin at the `(None, \"<START>\")` state and randomly sample a word from the list of words that follow this bigram. Then, at each step, we'll randomly sample the next word from the list of words that followed the current bigram (i.e., the last two words). We will continue this process until we sample the `\"<END>\"` state. This will give us the complete lyrics of a randomly generated song!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_new_lyrics(chain):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      - chain: a dict representing the Markov chain,\n",
    "               such as one generated by generate_new_lyrics()\n",
    "    \n",
    "    Returns:\n",
    "      A string representing the randomly generated song.\n",
    "    \"\"\"\n",
    "    \n",
    "    # a list for storing the generated words\n",
    "    words = []\n",
    "    # generate the first word\n",
    "    words.append(random.choice(chain[(None, \"<START>\")]))\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    # join the words together into a string with line breaks\n",
    "    lyrics = \" \".join(words[:-1])\n",
    "    return \"\\n\".join(lyrics.split(\"<N>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_new_lyrics(chain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "Compare the quality of the lyrics generated by the unigram model (in Lab B) and the bigram model (in Lab C). Which model seems to generate more reasonable lyrics? Can you explain why? What do you see as the advantages and disadvantages of each model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER HERE.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Instructions\n",
    "\n",
    "Once you are finished, follow these steps:\n",
    "\n",
    "1. Restart the kernel and re-run this notebook from beginning to end by going to `Kernel > Restart Kernel and Run All Cells`.\n",
    "2. If this process stops halfway through, that means there was an error. Correct the error and repeat Step 1 until the notebook runs from beginning to end.\n",
    "3. Double check that there is a number next to each code cell and that these numbers are in order.\n",
    "\n",
    "Then, submit your lab as follows:\n",
    "\n",
    "1. Go to `File > Export Notebook As > PDF`.\n",
    "2. Double check that the entire notebook, from beginning to end, is in this PDF file. (If the notebook is cut off, try first exporting the notebook to HTML and printing to PDF.)\n",
    "3. Upload the PDF [to PolyLearn](https://polylearn.calpoly.edu/AY_2018-2019/mod/assign/view.php?id=349486)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
